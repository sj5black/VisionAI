# VisionAI Pipeline - ì•„í‚¤í…ì²˜ ë¬¸ì„œ

## ğŸ¯ ì„¤ê³„ ì² í•™

### í•µì‹¬ ì›ì¹™

1. **ê²½ëŸ‰í™” ìš°ì„ **: ê° ë‹¨ê³„ë³„ë¡œ ê°€ì¥ ì‘ê³  ë¹ ë¥¸ ëª¨ë¸ ì„ íƒ
2. **ëª¨ë“ˆí™”**: ê° ë‹¨ê³„ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
3. **ì‹¤ìš©ì„±**: í•™ìŠµëœ ëª¨ë¸ ì—†ì´ë„ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘
4. **í™•ì¥ì„±**: í•™ìŠµëœ ëª¨ë¸ë¡œ ì‰½ê²Œ ëŒ€ì²´ ê°€ëŠ¥

---

## ğŸ“ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ì…ë ¥ ê³„ì¸µ                              â”‚
â”‚  Image/Video â†’ Frame Extraction â†’ RGB Array (H, W, 3)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Step 1-2: Detection Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  YOLOv8n (6.3 MB)                                    â”‚  â”‚
â”‚  â”‚  - Object Detection (COCO 80 classes)                â”‚  â”‚
â”‚  â”‚  - Keypoint Detection (17 points)                    â”‚  â”‚
â”‚  â”‚  - Output: Bbox, Class, Confidence, Keypoints        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Step 3: Emotion & Pose Layer                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MobileNetV3-Small (2.5 MB)                          â”‚  â”‚
â”‚  â”‚  - Feature Extraction                                 â”‚  â”‚
â”‚  â”‚  - Multi-task Head:                                   â”‚  â”‚
â”‚  â”‚    â€¢ Emotion: 5 classes                              â”‚  â”‚
â”‚  â”‚    â€¢ Pose: 5 classes                                 â”‚  â”‚
â”‚  â”‚  - Output: Emotion, Pose, Combined State             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Step 4: Temporal Analysis Layer                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Rule-based Temporal Aggregation                     â”‚  â”‚
â”‚  â”‚  - Temporal Buffer (deque, max 16 frames)            â”‚  â”‚
â”‚  â”‚  - Motion Intensity Calculation                       â”‚  â”‚
â”‚  â”‚  - Action Inference Rules                             â”‚  â”‚
â”‚  â”‚  - Output: Action, Duration, Motion Intensity        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Step 5: Behavior Prediction Layer                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LSTM Predictor (optional, lightweight)              â”‚  â”‚
â”‚  â”‚  - Action History Buffer (deque, max 8)              â”‚  â”‚
â”‚  â”‚  - State Transition Rules                             â”‚  â”‚
â”‚  â”‚  - Next Action Prediction                             â”‚  â”‚
â”‚  â”‚  - Output: Predicted Action, Confidence, Alts        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ì¶œë ¥ ê³„ì¸µ                               â”‚
â”‚  - JSON Results                                             â”‚
â”‚  - Visualization (optional)                                 â”‚
â”‚  - Metrics (processing time, FPS)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© ëª¨ë“ˆ ì„¤ê³„

### 1. ObjectDetector (`detection.py`)

**ì±…ì„**: ê°ì²´ íƒì§€ ë° í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ

```python
class ObjectDetector:
    - model: YOLOv8n (object detection)
    - pose_model: YOLOv8n-pose (keypoint detection)
    
    Methods:
    - detect(image) â†’ List[Detection]
    - detect_animals(image) â†’ List[Detection]
    - visualize(image, detections) â†’ np.ndarray
```

**í•µì‹¬ ê¸°ìˆ **:
- **YOLOv8n**: ê°€ì¥ ê²½ëŸ‰ ë²„ì „ (6.3 MB)
- **One-stage detector**: ë¹ ë¥¸ ì¶”ë¡  ì†ë„
- **Multi-scale detection**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ê°ì²´ íƒì§€
- **Keypoint detection**: 17ê°œ COCO í‚¤í¬ì¸íŠ¸

**ì„±ëŠ¥**:
- ì¶”ë¡  ì‹œê°„: ~20-30ms (GPU)
- FPS: ~30-50 (GPU)
- ì •í™•ë„: mAP 37.3 (COCO)

---

### 2. EmotionAnalyzer (`emotion.py`)

**ì±…ì„**: í‘œì • ë° ìì„¸ ë¶„ì„

```python
class EmotionAnalyzer:
    - model: EmotionClassifier (MobileNetV3 backbone)
    
    Methods:
    - analyze(image, bbox) â†’ EmotionResult
    - save_model(path)
```

**í•µì‹¬ ê¸°ìˆ **:
- **MobileNetV3-Small**: ê²½ëŸ‰ ë°±ë³¸ (2.5 MB)
- **Multi-task learning**: ê°ì • + ìì„¸ ë™ì‹œ í•™ìŠµ
- **Transfer learning**: ImageNet ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜

**í´ë˜ìŠ¤**:
- **Emotion**: relaxed, alert, fearful, aggressive, playful
- **Pose**: sitting, standing, lying, running, jumping

**ì„±ëŠ¥**:
- ì¶”ë¡  ì‹œê°„: ~5-10ms (GPU)
- íŒŒë¼ë¯¸í„°: ~1.5M

---

### 3. TemporalAnalyzer (`temporal.py`)

**ì±…ì„**: ì‹œê°„ íë¦„ ê¸°ë°˜ í–‰ë™ ì¸ì‹

```python
class TemporalAnalyzer:
    - feature_buffer: deque[TemporalFeature]
    - model: TemporalActionRecognizer (optional)
    
    Methods:
    - add_frame(timestamp, emotion, pose, ...)
    - analyze() â†’ ActionResult
    - reset()
```

**í•µì‹¬ ê¸°ìˆ **:
- **Temporal buffering**: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (16 í”„ë ˆì„)
- **Motion intensity**: Bbox ì¤‘ì‹¬ ì´ë™ ê±°ë¦¬ ê¸°ë°˜
- **Rule-based inference**: íœ´ë¦¬ìŠ¤í‹± ê·œì¹™

**í–‰ë™ í´ë˜ìŠ¤**:
- resting, eating, walking, running, playing, grooming, hunting, alert_scan

**ê·œì¹™ ì˜ˆì‹œ**:
```python
if motion_intensity < 0.1 and pose == 'lying':
    action = 'resting'
elif motion_intensity > 0.5 and emotion == 'playful':
    action = 'playing'
```

---

### 4. BehaviorPredictor (`predictor.py`)

**ì±…ì„**: ë‹¤ìŒ í–‰ë™ ì˜ˆì¸¡

```python
class BehaviorPredictor:
    - action_history: deque[str]
    - model: BehaviorPredictorModel (LSTM, optional)
    
    Methods:
    - add_action(action)
    - predict() â†’ PredictionResult
    - reset()
```

**í•µì‹¬ ê¸°ìˆ **:
- **State transition rules**: í–‰ë™ ì „ì´ í™•ë¥  í–‰ë ¬
- **LSTM (optional)**: ì‹œí€€ìŠ¤ í•™ìŠµ
- **Pattern detection**: ë°˜ë³µ íŒ¨í„´ ê°ì§€

**ì „ì´ ê·œì¹™ ì˜ˆì‹œ**:
```python
transitions = {
    'resting': {'resting': 0.6, 'walking': 0.2, 'grooming': 0.1},
    'playing': {'playing': 0.5, 'running': 0.2, 'resting': 0.1}
}
```

---

### 5. VisionAIPipeline (`pipeline.py`)

**ì±…ì„**: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¡°ìœ¨

```python
class VisionAIPipeline:
    - detector: ObjectDetector
    - emotion_analyzer: EmotionAnalyzer
    - temporal_analyzer: TemporalAnalyzer
    - predictor: BehaviorPredictor
    
    Methods:
    - process_image(image) â†’ PipelineResult
    - process_frame(image, timestamp) â†’ PipelineResult
    - visualize(image, result) â†’ np.ndarray
    - reset()
```

**íŠ¹ì§•**:
- **ëª¨ë“ˆí™”**: ê° ë‹¨ê³„ ë…ë¦½ì ìœ¼ë¡œ í™œì„±í™”/ë¹„í™œì„±í™”
- **ìƒíƒœ ê´€ë¦¬**: ì‹œê°„ ì¶• ì •ë³´ ìœ ì§€
- **ì—ëŸ¬ ì²˜ë¦¬**: ê° ë‹¨ê³„ë³„ graceful degradation

---

## ğŸ“Š ëª¨ë¸ í¬ê¸° ë° ì„±ëŠ¥ ë¹„êµ

### ìš”ì²­ëœ ëª¨ë¸ vs ì„ íƒëœ ëª¨ë¸

| ë‹¨ê³„ | ìš”ì²­ëœ ëª¨ë¸ | ì„ íƒëœ ëª¨ë¸ | í¬ê¸° | ì´ìœ  |
|------|------------|------------|------|------|
| 1-2 | YOLOv8/v9 | **YOLOv8n** | 6.3 MB | ê²½ëŸ‰í™” + í†µí•© (object + pose) |
| 3 | ViT/Swin/ConvNeXt | **MobileNetV3-Small** | 2.5 MB | í›¨ì”¬ ê²½ëŸ‰, ëª¨ë°”ì¼ ìµœì í™” |
| 4 | Video Swin/SlowFast | **ê·œì¹™ ê¸°ë°˜** | 0 MB | Heavy ëª¨ë¸ ë¶ˆí•„ìš”, ì‹¤ìš©ì„± |
| 5 | CNN/ViT + Head | **LSTM + ê·œì¹™** | <1 MB | ê²½ëŸ‰ ì‹œí€€ìŠ¤ ëª¨ë¸ |

**ì´ ëª¨ë¸ í¬ê¸°**: ~9-10 MB (YOLOv8 ê¸°ì¤€)

### ëŒ€ì•ˆ ëª¨ë¸ ë¹„êµ

#### Step 1-2: Object Detection

| ëª¨ë¸ | í¬ê¸° | mAP | ì†ë„ | ì„ íƒ ì´ìœ  |
|------|------|-----|------|-----------|
| **YOLOv8n** âœ“ | 6 MB | 37.3 | ë¹ ë¦„ | ìµœì  ê· í˜• |
| YOLOv8s | 22 MB | 44.9 | ì¤‘ê°„ | ë„ˆë¬´ í¼ |
| YOLOv9t | 4 MB | 38.3 | ë¹ ë¦„ | YOLOv8nê³¼ ìœ ì‚¬ |

#### Step 3: Feature Extractor

| ëª¨ë¸ | í¬ê¸° | ì •í™•ë„ | ì†ë„ | ì„ íƒ ì´ìœ  |
|------|------|--------|------|-----------|
| **MobileNetV3-Small** âœ“ | 2.5 MB | ì¤‘ìƒ | ë¹ ë¦„ | ê²½ëŸ‰, ëª¨ë°”ì¼ìš© |
| MobileNetV3-Large | 5.4 MB | ìƒ | ì¤‘ê°„ | ë¶ˆí•„ìš”í•˜ê²Œ í¼ |
| ViT-Tiny | 5.7 MB | ìƒ | ëŠë¦¼ | Attention ë¶ˆí•„ìš” |
| ConvNeXt-Tiny | 28 MB | ìµœìƒ | ëŠë¦¼ | ë„ˆë¬´ heavy |

#### Step 4-5: Temporal Models

| ì ‘ê·¼ë²• | ì¥ì  | ë‹¨ì  | ì„ íƒ |
|--------|------|------|------|
| **ê·œì¹™ ê¸°ë°˜** âœ“ | ë¹ ë¦„, í•´ì„ ê°€ëŠ¥ | ìœ ì—°ì„± ë‚®ìŒ | ê¸°ë³¸ |
| 1D Conv + Pooling | ì¤‘ê°„ ì†ë„ | í•™ìŠµ í•„ìš” | ì˜µì…˜ |
| LSTM | ì‹œí€€ìŠ¤ í•™ìŠµ | ëŠë¦¼, í•™ìŠµ í•„ìš” | ì˜µì…˜ |
| Video Swin | ìµœê³  ì •í™•ë„ | ë§¤ìš° heavy (>100MB) | âœ— |
| SlowFast | ì¢‹ì€ ì •í™•ë„ | Heavy (~30MB) | âœ— |

---

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

### ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬

```
Image (H,W,3)
    â”‚
    â–¼
[YOLOv8n Detection]
    â”‚
    â”œâ”€ Bbox 1 â†’ [Emotion Analysis] â†’ {emotion, pose}
    â”œâ”€ Bbox 2 â†’ [Emotion Analysis] â†’ {emotion, pose}
    â””â”€ Bbox N â†’ [Emotion Analysis] â†’ {emotion, pose}
    â”‚
    â–¼
PipelineResult {
    detections: [...],
    emotions: [...],
    action: None,
    prediction: None
}
```

### ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

```
Video Frames
    â”‚
    â”œâ”€ Frame 1 (t=0.0s)
    â”‚   â”œâ”€ Detection + Emotion
    â”‚   â””â”€ add_to_temporal_buffer()
    â”‚
    â”œâ”€ Frame 2 (t=0.2s)
    â”‚   â”œâ”€ Detection + Emotion
    â”‚   â””â”€ add_to_temporal_buffer()
    â”‚
    â”œâ”€ Frame 3 (t=0.4s)
    â”‚   â”œâ”€ Detection + Emotion
    â”‚   â”œâ”€ add_to_temporal_buffer()
    â”‚   â””â”€ [Temporal Analysis] â†’ {action}
    â”‚       â””â”€ add_to_predictor()
    â”‚
    â”œâ”€ Frame 4 (t=0.6s)
    â”‚   â””â”€ [Behavior Prediction] â†’ {next_action}
    â”‚
    â””â”€ ...
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. ëª¨ë¸ ìµœì í™”

```python
# TorchScript ë³€í™˜ (ì„ íƒ)
traced_model = torch.jit.trace(model, example_input)
traced_model.save("model_traced.pt")

# ì–‘ìí™” (ì„ íƒ)
quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
# YOLOv8 ë°°ì¹˜ ì¶”ë¡ 
results = model(image_list, batch=8)
```

### 3. í”„ë ˆì„ ìŠ¤í‚µ

```python
# 5 FPSë¡œ ìƒ˜í”Œë§ (30 FPS ë¹„ë””ì˜¤)
if frame_idx % 6 == 0:
    result = pipeline.process_frame(frame, timestamp)
```

### 4. ë¹„ë™ê¸° ì²˜ë¦¬

```python
import asyncio

async def process_video_async(frames):
    tasks = [pipeline.process_frame(f, t) for f, t in frames]
    results = await asyncio.gather(*tasks)
    return results
```

---

## ğŸ§ª í™•ì¥ ê°€ëŠ¥ì„±

### 1. ëª¨ë¸ êµì²´

```python
# ë” ì •í™•í•œ ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ
pipeline = VisionAIPipeline(
    device='cuda',
    emotion_model_path='trained_emotion_model.pth',
    temporal_model_path='trained_temporal_model.pth'
)
```

### 2. ì»¤ìŠ¤í…€ í´ë˜ìŠ¤

```python
# ìƒˆë¡œìš´ ê°ì • í´ë˜ìŠ¤ ì¶”ê°€
EMOTION_CLASSES = ['happy', 'sad', 'angry', 'neutral', 'surprised']
```

### 3. ì•™ìƒë¸”

```python
# ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©
result1 = pipeline1.process_image(image)
result2 = pipeline2.process_image(image)
final_result = ensemble([result1, result2])
```

### 4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

```python
# WebRTC ë˜ëŠ” RTSP ìŠ¤íŠ¸ë¦¼
import cv2

cap = cv2.VideoCapture('rtsp://camera_ip/stream')
while True:
    ret, frame = cap.read()
    result = pipeline.process_frame(frame, time.time())
    # ê²°ê³¼ë¥¼ WebSocketìœ¼ë¡œ ì „ì†¡
```

---

## ğŸ“ˆ í–¥í›„ ê°œì„  ë°©í–¥

1. **ëª¨ë¸ í•™ìŠµ**: ì‹¤ì œ ë™ë¬¼ ë°ì´í„°ì…‹ìœ¼ë¡œ fine-tuning
2. **ì¢… íŠ¹í™”**: ê°œ/ê³ ì–‘ì´ ê°ê°ì— ìµœì í™”ëœ ëª¨ë¸
3. **3D Pose**: Depth ì •ë³´ í™œìš©
4. **ë©€í‹° ê°ì²´**: ì—¬ëŸ¬ ë™ë¬¼ ê°„ ìƒí˜¸ì‘ìš© ë¶„ì„
5. **ì—£ì§€ ë°°í¬**: TensorRT, ONNX ë³€í™˜

---

## ğŸ”’ ì œì•½ì‚¬í•­

1. **í•™ìŠµ ë°ì´í„° ë¶€ì¡±**: í˜„ì¬ ê·œì¹™ ê¸°ë°˜ (í•™ìŠµ ì‹œ ê°œì„  ê°€ëŠ¥)
2. **ë‹¨ì¼ ê°ì²´ ì¶”ì **: ì—¬ëŸ¬ ê°ì²´ ì‹œ ê°ê° ë…ë¦½ ë¶„ì„
3. **2D ì •ë³´ë§Œ**: Depth ì •ë³´ ì—†ìŒ
4. **í•´ì„ ê°€ëŠ¥ì„±**: ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ë¸”ë™ë°•ìŠ¤ íŠ¹ì„±

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [MobileNetV3 Paper](https://arxiv.org/abs/1905.02244)
- [Animal Pose Estimation Survey](https://arxiv.org/abs/2103.05644)

---

**ë²„ì „**: 1.0.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-02-02
