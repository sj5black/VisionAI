# VisionAI Pipeline - ë™ë¬¼ í–‰ë™ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

ê²½ëŸ‰í™”ëœ 5ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë™ë¬¼ì˜ í–‰ë™ì„ ë¶„ì„í•˜ê³  ì˜ˆì¸¡í•©ë‹ˆë‹¤.

## ğŸ“‹ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VisionAI Pipeline                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 1-2   â”‚    â”‚   Step 3     â”‚    â”‚   Step 4-5   â”‚
â”‚              â”‚    â”‚              â”‚    â”‚              â”‚
â”‚  Detection   â”‚â”€â”€â”€â–¶â”‚   Emotion    â”‚â”€â”€â”€â–¶â”‚  Temporal    â”‚
â”‚  + Keypoint  â”‚    â”‚   Analysis   â”‚    â”‚  + Predict   â”‚
â”‚              â”‚    â”‚              â”‚    â”‚              â”‚
â”‚   YOLOv8n    â”‚    â”‚ MobileNetV3  â”‚    â”‚ Rule + LSTM  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ ê° ë‹¨ê³„ ìƒì„¸

#### Step 1-2: Object + Keypoint Detection (YOLOv8n)
- **ëª¨ë¸**: YOLOv8n (6.3 MB) - ê°€ì¥ ê²½ëŸ‰ ëª¨ë¸
- **ê¸°ëŠ¥**:
  - ê°œ/ê³ ì–‘ì´ ë“± ë™ë¬¼ íƒì§€ ë° ìœ„ì¹˜ íŒŒì•…
  - ì‹ ì²´ ë¶€ìœ„ keypoint íƒì§€ (17ê°œ í¬ì¸íŠ¸)
- **ì…ë ¥**: RGB ì´ë¯¸ì§€
- **ì¶œë ¥**: ê°ì²´ ìœ„ì¹˜ (bbox), í´ë˜ìŠ¤, keypoints

#### Step 3: Emotion & Pose Analysis (MobileNetV3-Small)
- **ëª¨ë¸**: MobileNetV3-Small (2.5 MB) + ë©€í‹°íƒœìŠ¤í¬ í—¤ë“œ
- **ê¸°ëŠ¥**:
  - **í‘œì • ë¶„ì„**: relaxed, alert, fearful, aggressive, playful
  - **ìì„¸ ë¶„ì„**: sitting, standing, lying, running, jumping
  - í†µí•© ìƒíƒœ íŒë‹¨
- **ì…ë ¥**: íƒì§€ëœ ê°ì²´ ì˜ì—­ (cropped)
- **ì¶œë ¥**: ê°ì •, ìì„¸, í†µí•© ìƒíƒœ

#### Step 4: Temporal Action Recognition (ê·œì¹™ ê¸°ë°˜)
- **ë°©ë²•**: ì‹œê°„ ì¶• íŠ¹ì§• ì§‘ê³„ + ê·œì¹™ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
- **ê¸°ëŠ¥**:
  - ì—¬ëŸ¬ í”„ë ˆì„ì˜ ê°ì •/ìì„¸ ë³€í™” ì¶”ì 
  - ì›€ì§ì„ ê°•ë„ ê³„ì‚°
  - í–‰ë™ ì¸ì‹: resting, walking, running, playing, eating, grooming, hunting, alert_scan
- **ì…ë ¥**: ì‹œê°„ ìˆœì„œ íŠ¹ì§• ì‹œí€€ìŠ¤
- **ì¶œë ¥**: í˜„ì¬ í–‰ë™, ì§€ì† ì‹œê°„, ì›€ì§ì„ ê°•ë„

#### Step 5: Behavior Prediction (ê·œì¹™ ê¸°ë°˜ + LSTM)
- **ëª¨ë¸**: ê²½ëŸ‰ LSTM (ì˜µì…˜)
- **ê¸°ëŠ¥**:
  - ê³¼ê±° í–‰ë™ íŒ¨í„´ ë¶„ì„
  - ë‹¤ìŒ í–‰ë™ ì˜ˆì¸¡ (5ì´ˆ í›„)
  - ëŒ€ì•ˆ í–‰ë™ ì œì‹œ
- **ì…ë ¥**: í–‰ë™ ì‹œí€€ìŠ¤
- **ì¶œë ¥**: ì˜ˆì¸¡ í–‰ë™, ì‹ ë¢°ë„, ëŒ€ì•ˆ

## ğŸš€ ì„¤ì¹˜

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd /home/teddy/VisionAI

# íŒŒì´í”„ë¼ì¸ ì „ìš© requirements
pip install -r pipeline_requirements.txt
```

### 2. YOLOv8 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

ì²« ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤ (~9 MB):
- `yolov8n.pt` (object detection)
- `yolov8n-pose.pt` (keypoint detection)

## ğŸ“– ì‚¬ìš©ë²•

### CLIë¡œ ì´ë¯¸ì§€ ë¶„ì„

```bash
# ê¸°ë³¸ ì‚¬ìš©
python run_pipeline.py --image dog.jpg --output result.jpg

# ë””ë°”ì´ìŠ¤ ì§€ì •
python run_pipeline.py --image cat.jpg --output result.jpg --device cuda

# ì‹ ë¢°ë„ ì¡°ì •
python run_pipeline.py --image pet.jpg --conf 0.7
```

### CLIë¡œ ë¹„ë””ì˜¤ ë¶„ì„

```bash
# ë¹„ë””ì˜¤ ë¶„ì„ (5 FPS ìƒ˜í”Œë§)
python run_pipeline.py --video cat_video.mp4 --output result.mp4 --fps 5

# ë¹ ë¥¸ ìƒ˜í”Œë§ (1 FPS)
python run_pipeline.py --video dog_video.mp4 --output result.mp4 --fps 1

# ì‹œê°í™” ì—†ì´ JSONë§Œ ì €ì¥
python run_pipeline.py --video video.mp4 --output result.json --no-visualize
```

### Python ì½”ë“œì—ì„œ ì‚¬ìš©

```python
from visionai_pipeline import VisionAIPipeline
import numpy as np
from PIL import Image

# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
pipeline = VisionAIPipeline(device='cuda')

# ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
image = np.array(Image.open('dog.jpg'))
result = pipeline.process_image(image)

print(f"íƒì§€: {len(result.detections)}ê°œ")
print(f"ê°ì •: {result.emotions}")
print(f"í–‰ë™: {result.action}")
print(f"ì˜ˆì¸¡: {result.prediction}")

# ì‹œê°í™”
vis_image = pipeline.visualize(image, result)
```

### ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

```python
import cv2
from visionai_pipeline import VisionAIPipeline

pipeline = VisionAIPipeline(device='cuda')
cap = cv2.VideoCapture('video.mp4')

frame_idx = 0
while True:
    ret, frame_bgr = cap.read()
    if not ret:
        break
    
    # BGR -> RGB
    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = frame_idx / 30.0  # 30 FPS ê°€ì •
    
    # ë¶„ì„
    result = pipeline.process_frame(frame_rgb, timestamp)
    
    # ê²°ê³¼ ì¶œë ¥
    if result.prediction:
        print(f"ë‹¤ìŒ í–‰ë™ ì˜ˆì¸¡: {result.prediction['predicted_action']}")
    
    frame_idx += 1

cap.release()
```

## âš™ï¸ ê²½ëŸ‰í™” ì˜µì…˜

í•„ìš”ì— ë”°ë¼ ì¼ë¶€ ë‹¨ê³„ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë” ë¹ ë¥´ê²Œ ì‹¤í–‰:

```bash
# ê°ì • ë¶„ì„ë§Œ ë¹„í™œì„±í™”
python run_pipeline.py --image dog.jpg --no-emotion

# ì‹œê°„ ì¶• ë¶„ì„ ë¹„í™œì„±í™” (ë‹¨ì¼ ì´ë¯¸ì§€ì— ì í•©)
python run_pipeline.py --image dog.jpg --no-temporal --no-prediction

# ìµœì†Œ ëª¨ë“œ (íƒì§€ë§Œ)
python run_pipeline.py --image dog.jpg --no-emotion --no-temporal --no-prediction
```

Pythonì—ì„œ:

```python
# ìµœì†Œ êµ¬ì„± (íƒì§€ë§Œ)
pipeline = VisionAIPipeline(
    device='cuda',
    enable_emotion=False,
    enable_temporal=False,
    enable_prediction=False
)

# ê°ì • ë¶„ì„ê¹Œì§€ë§Œ
pipeline = VisionAIPipeline(
    device='cuda',
    enable_temporal=False,
    enable_prediction=False
)
```

## ğŸ“Š ì¶œë ¥ í˜•ì‹

### JSON êµ¬ì¡°

```json
{
  "detections": [
    {
      "class_id": 16,
      "class_name": "dog",
      "confidence": 0.92,
      "bbox": [100, 150, 400, 500],
      "has_keypoints": true
    }
  ],
  "emotions": [
    {
      "class_name": "dog",
      "emotion": "playful",
      "emotion_confidence": 0.85,
      "pose": "running",
      "pose_confidence": 0.91,
      "combined_state": "playing"
    }
  ],
  "action": {
    "action": "playing",
    "confidence": 0.7,
    "duration": 2.5,
    "motion_intensity": 0.8
  },
  "prediction": {
    "predicted_action": "resting",
    "confidence": 0.65,
    "time_horizon": 5.0,
    "alternative_actions": [
      ["walking", 0.25],
      ["playing", 0.10]
    ]
  },
  "timestamp": 1234567890.123,
  "processing_time": 0.15
}
```

## ğŸ“ ëª¨ë¸ í•™ìŠµ (ì„ íƒ)

í˜„ì¬ëŠ” ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ì§€ë§Œ, ë°ì´í„°ê°€ ìˆë‹¤ë©´ í•™ìŠµ ê°€ëŠ¥:

### Step 3: ê°ì • ë¶„ì„ ëª¨ë¸ í•™ìŠµ

```python
from visionai_pipeline.emotion import EmotionAnalyzer, EmotionClassifier
import torch
from torch.utils.data import DataLoader

# ëª¨ë¸ ì´ˆê¸°í™”
model = EmotionClassifier(num_emotions=5, num_poses=5)

# í•™ìŠµ ë£¨í”„ (ì˜ˆì‹œ)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    for images, emotion_labels, pose_labels in train_loader:
        optimizer.zero_grad()
        
        emotion_logits, pose_logits = model(images)
        
        loss = (criterion(emotion_logits, emotion_labels) + 
                criterion(pose_logits, pose_labels))
        
        loss.backward()
        optimizer.step()

# ì €ì¥
torch.save(model.state_dict(), 'emotion_model.pth')

# ì‚¬ìš©
pipeline = VisionAIPipeline(emotion_model_path='emotion_model.pth')
```

## ğŸ”§ ì„±ëŠ¥ ìµœì í™”

### GPU ì‚¬ìš©

```python
# CUDA
pipeline = VisionAIPipeline(device='cuda')

# Apple Silicon (MPS)
pipeline = VisionAIPipeline(device='mps')
```

### ë°°ì¹˜ ì²˜ë¦¬

ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë ¤ë©´ YOLOv8ì˜ ë°°ì¹˜ ê¸°ëŠ¥ í™œìš©:

```python
from ultralytics import YOLO

model = YOLO('yolov8n.pt')
results = model(image_list, batch=8)  # ë°°ì¹˜ í¬ê¸° 8
```

### ëª¨ë¸ í¬ê¸° vs ì •í™•ë„

| ëª¨ë¸ | í¬ê¸° | ì†ë„ | ì •í™•ë„ |
|------|------|------|--------|
| YOLOv8n | 6 MB | ë§¤ìš° ë¹ ë¦„ | ì–‘í˜¸ |
| YOLOv8s | 22 MB | ë¹ ë¦„ | ì¢‹ìŒ |
| YOLOv8m | 52 MB | ì¤‘ê°„ | ë§¤ìš° ì¢‹ìŒ |

í˜„ì¬ íŒŒì´í”„ë¼ì¸ì€ YOLOv8nì„ ì‚¬ìš©í•˜ì—¬ **ê²½ëŸ‰í™”**ë¥¼ ìš°ì„ ì‹œí•©ë‹ˆë‹¤.

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
VisionAI/
â”œâ”€â”€ visionai_pipeline/           # íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py              # í†µí•© íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ detection.py             # Step 1-2: ê°ì²´+í‚¤í¬ì¸íŠ¸ íƒì§€
â”‚   â”œâ”€â”€ emotion.py               # Step 3: ê°ì • ë¶„ì„
â”‚   â”œâ”€â”€ temporal.py              # Step 4: ì‹œê°„ ì¶• í–‰ë™ ì¸ì‹
â”‚   â””â”€â”€ predictor.py             # Step 5: í–‰ë™ ì˜ˆì¸¡
â”‚
â”œâ”€â”€ run_pipeline.py              # CLI ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ pipeline_requirements.txt    # ì˜ì¡´ì„±
â”œâ”€â”€ PIPELINE_README.md          # ì´ ë¬¸ì„œ
â”‚
â””â”€â”€ visionai_resnet/            # ê¸°ì¡´ ResNet ê¸°ë°˜ (í˜¸í™˜ì„± ìœ ì§€)
    â””â”€â”€ ...
```

## ğŸ› ë¬¸ì œ í•´ê²°

### YOLOv8 ì„¤ì¹˜ ì˜¤ë¥˜

```bash
# ultralytics ì¬ì„¤ì¹˜
pip uninstall ultralytics -y
pip install ultralytics --no-cache-dir
```

### CUDA Out of Memory

```bash
# CPU ì‚¬ìš©
python run_pipeline.py --image dog.jpg --device cpu

# ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ:
```bash
wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt
wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **ë°ì´í„° ìˆ˜ì§‘**: ë™ë¬¼ í–‰ë™ ë°ì´í„°ì…‹ ìˆ˜ì§‘
2. **ëª¨ë¸ í•™ìŠµ**: ê°ì •/í–‰ë™ ë¶„ë¥˜ê¸° í•™ìŠµ
3. **Fine-tuning**: íŠ¹ì • ë™ë¬¼ ì¢…ì— ë§ê²Œ ì¡°ì •
4. **ë°°í¬**: ì›¹ API ë˜ëŠ” ëª¨ë°”ì¼ ì•±ìœ¼ë¡œ ë°°í¬

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ê¸°ì¡´ VisionAI í”„ë¡œì íŠ¸ì˜ í™•ì¥ì…ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê¸°ëŠ¥ ì œì•ˆì€ ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”.
